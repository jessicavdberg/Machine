\documentclass[11pt,preprint, authoryear]{elsarticle}

\usepackage{lmodern}
%%%% My spacing
\usepackage{setspace}
\setstretch{1.2}
\DeclareMathSizes{12}{14}{10}{10}

% Wrap around which gives all figures included the [H] command, or places it "here". This can be tedious to code in Rmarkdown.
\usepackage{float}
\let\origfigure\figure
\let\endorigfigure\endfigure
\renewenvironment{figure}[1][2] {
    \expandafter\origfigure\expandafter[H]
} {
    \endorigfigure
}

\let\origtable\table
\let\endorigtable\endtable
\renewenvironment{table}[1][2] {
    \expandafter\origtable\expandafter[H]
} {
    \endorigtable
}


\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{â‚¬}
\fi

\usepackage{amssymb, amsmath, amsthm, amsfonts}

\def\bibsection{\section*{References}} %%% Make "References" appear before bibliography


\usepackage[round]{natbib}

\usepackage{longtable}
\usepackage[margin=2.3cm,bottom=2cm,top=2.5cm, includefoot]{geometry}
\usepackage{fancyhdr}
\usepackage[bottom, hang, flushmargin]{footmisc}
\usepackage{graphicx}
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\numberwithin{table}{section}
\setlength{\parindent}{0cm}
\setlength{\parskip}{1.3ex plus 0.5ex minus 0.3ex}
\usepackage{textcomp}
\renewcommand{\headrulewidth}{0.2pt}
\renewcommand{\footrulewidth}{0.3pt}

\usepackage{array}
\newcolumntype{x}[1]{>{\centering\arraybackslash\hspace{0pt}}p{#1}}

%%%%  Remove the "preprint submitted to" part. Don't worry about this either, it just looks better without it:
\makeatletter
\def\ps@pprintTitle{%
  \let\@oddhead\@empty
  \let\@evenhead\@empty
  \let\@oddfoot\@empty
  \let\@evenfoot\@oddfoot
}
\makeatother

 \def\tightlist{} % This allows for subbullets!

\usepackage{hyperref}
\hypersetup{breaklinks=true,
            bookmarks=true,
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=blue,
            pdfborder={0 0 0}}


% The following packages allow huxtable to work:
\usepackage{siunitx}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{calc}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{caption}


\newenvironment{columns}[1][]{}{}

\newenvironment{column}[1]{\begin{minipage}{#1}\ignorespaces}{%
\end{minipage}
\ifhmode\unskip\fi
\aftergroup\useignorespacesandallpars}

\def\useignorespacesandallpars#1\ignorespaces\fi{%
#1\fi\ignorespacesandallpars}

\makeatletter
\def\ignorespacesandallpars{%
  \@ifnextchar\par
    {\expandafter\ignorespacesandallpars\@gobble}%
    {}%
}
\makeatother

\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newenvironment{CSLReferences}%
  {\setlength{\parindent}{0pt}%
  \everypar{\setlength{\hangindent}{\cslhangindent}}\ignorespaces}%
  {\par}


\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{5}

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}

%%% Include extra packages specified by user
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}

%%% Hard setting column skips for reports - this ensures greater consistency and control over the length settings in the document.
%% page layout
%% paragraphs
\setlength{\baselineskip}{12pt plus 0pt minus 0pt}
\setlength{\parskip}{12pt plus 0pt minus 0pt}
\setlength{\parindent}{0pt plus 0pt minus 0pt}
%% floats
\setlength{\floatsep}{12pt plus 0 pt minus 0pt}
\setlength{\textfloatsep}{20pt plus 0pt minus 0pt}
\setlength{\intextsep}{14pt plus 0pt minus 0pt}
\setlength{\dbltextfloatsep}{20pt plus 0pt minus 0pt}
\setlength{\dblfloatsep}{14pt plus 0pt minus 0pt}
%% maths
\setlength{\abovedisplayskip}{12pt plus 0pt minus 0pt}
\setlength{\belowdisplayskip}{12pt plus 0pt minus 0pt}
%% lists
\setlength{\topsep}{10pt plus 0pt minus 0pt}
\setlength{\partopsep}{3pt plus 0pt minus 0pt}
\setlength{\itemsep}{5pt plus 0pt minus 0pt}
\setlength{\labelsep}{8mm plus 0mm minus 0mm}
\setlength{\parsep}{\the\parskip}
\setlength{\listparindent}{\the\parindent}
%% verbatim
\setlength{\fboxsep}{5pt plus 0pt minus 0pt}



\begin{document}



\begin{frontmatter}  %

\title{Predicting Poverty levels in South Africa}

% Set to FALSE if wanting to remove title (for submission)




\author[Add1]{Jessica van der Berg}
\ead{20190565@sun.ac.za}





\address[Add1]{Stellenbosch University, South Africa}


\begin{abstract}
\small{
Abstract to be written here. The abstract should not be too long and
should provide the reader with a good understanding what you are writing
about. Academic papers are not like novels where you keep the reader in
suspense. To be effective in getting others to read your paper, be as
open and concise about your findings here as possible. Ideally, upon
reading your abstract, the reader should feel he / she must read your
paper in entirety.
}
\end{abstract}

\vspace{1cm}

\begin{keyword}
\footnotesize{
Multivariate GARCH \sep Kalman Filter \sep Copula \\ \vspace{0.3cm}
\textit{JEL classification} L250 \sep L100
}
\end{keyword}
\vspace{0.5cm}
\end{frontmatter}



%________________________
% Header and Footers
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{fancy}
\chead{}
\rhead{}
\lfoot{}
\rfoot{\footnotesize Page \thepage}
\lhead{}
%\rfoot{\footnotesize Page \thepage } % "e.g. Page 2"
\cfoot{}

%\setlength\headheight{30pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%________________________

\headsep 35pt % So that header does not go over title




\hypertarget{introduction}{%
\section{\texorpdfstring{Introduction
\label{Introduction}}{Introduction }}\label{introduction}}

Since apartheid ended in 1994, the South African Government has
committed a significant number of resources to distribute grants
effectively and efficiently to the poor and the vulnerable. However,
South Africa has remained a country with extremely high levels of
inequality and poverty for an upper middle-income country
\protect\hyperlink{ref-stats2011social}{Stats}
(\protect\hyperlink{ref-stats2011social}{2011}). Previous literature has
shown that poverty levels are higher in rural areas than in urban areas.
This is largely due to rural household not having access to the same
employment opportunities has urban households. South Africa is a country
with high levels of unemployment and extremely low wages. This implies
that individuals that are economically active can still fall below the
upper or lower bound poverty line due to the low wages that they receive
\protect\hyperlink{ref-leibbrandt2010trends}{Leibbrandt, Woolard, Finn
\& Argent} (\protect\hyperlink{ref-leibbrandt2010trends}{2010}).

Poverty remains unacceptably high for a country of South Africa's
economic status and remains closely associated with race. Thus, poverty
reduction remains one of the key economic goals. Poverty in money terms
has declined markedly since apartheid ended in 1994. This was made
possible by the expansion of the social grant system. However,
accurately targeting social welfare programs can be challenging given
that income data is often incorrect
\protect\hyperlink{ref-vanderberg2017}{Yu \& Van der Berg}
(\protect\hyperlink{ref-vanderberg2017}{2017}). To overcome this
problem, households are subject to a proxy means test (PMT) to identify
whether a household qualifies for social assistant. This essay will try
to identify new methods to identify households that are below the food
poverty line, lower-bound poverty line and upper bound poverty line
using machine learning techniques.

Structure \ldots{} `

\hypertarget{literature-review}{%
\section{Literature Review}\label{literature-review}}

Write a short literature review here on the background of SOuth Africa -
max one page

\hypertarget{data}{%
\section{Data}\label{data}}

The data used for predicting poverty level was exacted from the General
Household Survey (GHS) 2018, which is a survey completed annually by
Statistics SA to measure the living circumstances of households in South
Africa. They survey includes household and individual characteristics.
After taking out all of the NA values and taking out households who did
not know or answer the relevant questions, the dataset consists of 14
546 entries.

Since the GHS consist of household survey, total income per household is
reported. To calculate the average monthly income per individual within
each household, I first need to calculate the total number of adults
within each household. This is done by taking the difference between
household size and the number of children under the age of 17. I then
take the total monthly income and divide it by the total number of
adults to get the average monthly income per individual in each
household. This income information is then used to divided the data into
four groups.

The first group is individuals whose income fall below the food poverty
line. In 2018, the food poverty line was R547 per month. The food
poverty line is also referred to as the extreme poverty line as it
refers to the absolute minimum amount an individual will need to be able
to afford the minimum energy intake for survival. The second group
consist of individuals whose monthly income falls between the food
poverty line and the lower-bound poverty line (R785). The lower bound
poverty line is the sum of the food poverty line and and minimum amount
for non-food items. The third group consist of individuals between the
lower-bound poverty line and the upper-bound poverty line (R1183). The
final group consist of individuals whose monthly income is above the
upper-bound poverty line, which I refer to as non-vulnerable
individuals. Figure 1 graphically displays the process described above
in the format of a decision tree, where 1 represents the food poverty
line, 2 the lower-bound poverty line, 3 the upper-bound poverty line and
4 the non-vulnerable.

\begin{figure}[H]

{\centering \includegraphics{Predicting-Poverty_files/figure-latex/Figure1-1} 

}

\caption{Decision tree for poverty levels \label{Figure1}}\label{fig:Figure1}
\end{figure}

After all data cleaning was done, 73 percent of households were
non-vulnerable, 11 percent were between the upper and lower-bound
poverty line, 6 percent between the lower and food poverty line, and 9
percent fell below the food poverty line. To evaluate the performance
for the machine learning techniques that I will implement, I randomly
split the data into two subsets using a 70:30 ratio. The training
dataset will consist of 70 percent of the original dataset, while the
test dataset will consist of the remaining 30 percent. Figure 2 below
shows the number of household per poverty level for the training
dataset.

\begin{figure}
\centerline{\includegraphics[scale=0.65]{plot1.png}}
\caption{Number of household per poverty level}
\end{figure}

Figure 2 shows that in our training dataset, 73.5 percent of households
are non-vulnerable and 9.6 percent of households monthly income falls
below the food poverty line. This implies that I have an extremely
imbalanced dataset. This is important as it will have an effect on the
machine learning techniques that I implement later on.

For some descriptive statistics, I analyze some of the variable in more
details. From the density plots in figure 3.3, I can see that households
whose average income per person falls below the upper-bound poverty line
(green line) tend to have slightly older head of household (head\_age)
then those household belonging to other poverty levels.

A household size (hholdsz) of 1-2 individuals tend have a larger
probability to be non-vulnerable (yellow line), where larger household
size of 3-5 individuals then have a larger probability of falling below
the food poverty line.

Total expenditure (Q814Exp) is higher for the non-vulnerable households,
which makes sense as they receive a higher income.

The total number of rooms (Q55TotRm) seem to be distributed similarly,
with non-vulnerable households having a slightly smaller distribution.
This comes as no suprise as these non-vulnerable households tend to have
a smaller household size, implying that they need less bedrooms.

Non-vulnerable household also have much larger property valuation
(Q58Val), which could imply that they have a higher standard of living.

\begin{figure}[H]

{\centering \includegraphics{Predicting-Poverty_files/figure-latex/Figure3-1} 

}

\caption{Distribution of certain variables by poverty level \label{Figure3}}\label{fig:Figure3}
\end{figure}

Before we start building models we can also analysis which variables are
correlated with our dependent variable (which is is poverty level).
Figure 3.4 displays the top 10 variables that are correlated with the
dependent variable. Our top four that are positively correlated are
\textit{econact hh}, which is a binary variable indicating whether or
not the household is economically active, \emph{Q814Exp}, which is a
numeric variable display total household expenditure, \emph{totmhinc},
which is a numeric variable displaying total household income, and
\emph{Q89aGrant}, which is a binary variable indicating whether the
household receives a grant or not.

variables that are negatively correlated with the dependent variable is
the sex of the head of the household (\emph{head sex}), whether or not
the household owns a washing machine (\emph{Q821WashM}), the household
size (\emph{hholdsz}), the amount spent on rent or mortgage of property
per month (\emph{Q57Rent}) and the number of adults in the household
(\emph{adults}).

\begin{figure}[H]

{\centering \includegraphics{Predicting-Poverty_files/figure-latex/Figure4-1} 

}

\caption{Correlation with dependent variable \label{Figure4}}\label{fig:Figure4}
\end{figure}

\hypertarget{methodology}{%
\section{\texorpdfstring{Methodology
\label{Meth}}{Methodology }}\label{methodology}}

\hypertarget{analysing-classification-models}{%
\subsection{Analysing Classification
Models}\label{analysing-classification-models}}

To see which model predicts poverty most accurately and efficiently, I
assess the performance of x supervised classification models using the
caret package. The dataset is extremely unbalanced; therefore, the
defaults parameter will be Kappa to order to improve the performance of
the models. The Kappa metric compares the observed accuracy with the
expected accuracy. It also accounts for random chance which implies that
it makes the model more accurate that simply using an Accuracy as a
metric. The Kappa matric is calculated using the formula;
\[k = \frac{p_0 -p_e}{1-p_e}\] Where \(p_0\) represents the overall
accuracy of the model and \(p_e\) represents that measure of the
agreement between the predictions and actual class value of the model.
Therefore, Kappa attempts to account for evaluation bias by considering
the correct classification by a random guess.

The table below briefly discusses the seven different classification
models that I compared to determine which model is the most accurate to
predict poverty \protect\hyperlink{ref-boehmke2019hands}{Boehmke \&
Greenwell} (\protect\hyperlink{ref-boehmke2019hands}{2019}).

\begin{center}
\begin{tabular}{| m{10em} | m{2cm}| m{10cm} |}
\hline
\textbf{Model} & \textbf{Reference Name} & \textbf{Description} \\
\hline
Multinomial Logistic Regression &  multinom & Makes use of the maximum likelihood estimation to evaluate the probability of a categorical relationship \\
\hline
Linear Discriminant Analysis & lda & Used to find linear combinations of separates multiple classes of features, representing the dependent variable as a linear combination of other features \\
\hline
Naive Bayes & naive bayes & Using Bayes Theorem, this model applies posterior probability to the categorization, making the uneducated assumption that the predictors are independent \\
\hline
Linear Support Vector Machine & svmLinear & The model creates a line that separates data into classes \\
\hline
K-Nearest Neighbor & knn & Each observation in the dataset is predicted based on its similarity to other observations \\
\hline
Recursive Partitioning & rpart & Builds models using a general structure which consist of a two-stage procedure and then final presenting the model as a binary tree \\
\hline 
Ranger & ranger & An updated and fast implementation of random forest for big data \\
\hline
\end{tabular}
\end{center}

\hypertarget{evaluation}{%
\subsubsection{Evaluation}\label{evaluation}}

To ensure that the correct model is chosen, the accuracy of each model
is evaluated and compared. However, since the dataset is unbalance, I
also analyze the F measure, also known as the \(F_1\) score, of each
model. The \(F_1\) score communicates the average between the precision
and the recall of each model. A perfect model has an \(F_1\) score equal
to 1, therefore models with a higher \(F_1\) score is preferred over
models with a lower \(F_1\) score. The formula for the \(F_1\) score is
given below;
\[F_1 = 2\times \frac{ precision \times recall}{precision + recall}\]

The extremely unbalanced data will affect the results of the \(F_1\)
score. Therefore, it is also informative to evaluate the macro \(F_1\)
score and the weighted \(F_1\) score. The macro \(F_1\) is not affected
by unbalanced data and is equal to the average of the \(F_1\) score and
is commonly used when there are multiple levels or classes. It gives the
same importance to each poverty level. A higher macro \(F_1\) score is
preferred to a lower one. The formula for the macro \(F_1\) score if
given below; \[ Macro\: F_1\: score = \frac{1}{N}{\sum}_{i=0}^N F_1\]

Where \(N\) is the number of different poverty levels and \(i\) is the
levels index. The drawback of the macro \(F_1\) measure is that is gives
equal weight to all poverty levels, which implies that it over emphasis
the under-represented poverty levels. The weighted average \(F_1\) score
is similar to the macro \(F_1\) score, but here the \(F_1\) score is
weighed according of the number of households from the specific poverty
level, which emphasis poverty levels according to size of each poverty
level. The formula for the weighted \(F_1\) score is given below;
\[ Weighted\: F_1\: score = \frac{n_i\sum_{i=1}^k F_1}{\sum_{i=n}^k n_i}\]

\hypertarget{results-and-discussion}{%
\section{Results and Discussion}\label{results-and-discussion}}

\hypertarget{classification-models}{%
\subsection{Classification Models}\label{classification-models}}

The table below shows the different metrics to make comparing different
models easier. The first feature that is observed is that the models
vary drastically in time. The \emph{lda}, \emph{rpart} and
\emph{naive bayes} models are extremely fast whereas \emph{multinom} and
\emph{ranger} take relatively long to run. Furthermore, \emph{ranger}
scores the best for accuracy and \(F_1\) measures.

\begin{center}
\begin{tabular}{| c| c| c |c| c|}
\hline
\textbf{Models} & \textbf{Time} & \textbf{Accuracy} & \textbf{Macro \(F_1\)} & \textbf{weighted \(F_1\)} \\
\hline
multinom & 37.17 & 0.9935839 & 0.9820473 & 0.9935798 \\
\hline
lda & 0.81 & 0.7774977 & 0.4561088 & 0.7409658 \\
\hline
naive bayes & 1.67 & 0.8819890 & 0.8138291 & 0.8862441 \\
\hline
svmLinear & 9.91 & 0.9660862 & 0.9134729 & 0.9661192 \\
\hline
knn & 4.81 & 0.9869386 & 0.9714937 & 0.9876236 \\
\hline 
rpart & 1.08 & 0.9372136 & 0.9155884 & 0.9137702 \\
\hline 
ranger & 41.92 & 1.00000000 & 1.00000000 & 1.00000000 \\
\hline
\end{tabular}
\end{center}

Analyzing the accuracy score, Figure 5.1 shows that \emph{ranger} has
the highest accuracy, followed closely by \emph{multinom}. However,
these two models also take the longest to run. \emph{knn} and
\textbackslash emph\{rpart) are relatively fast and thet have a high
level of accuracy. For the fastest running models, decision trees
(\emph{rpart}) are the most accurate. Figure 5.1 also implies that there
is some trade-off between accuracy and computational time.

\begin{figure}
\centerline{\includegraphics[scale=0.65]{accuracy.png}}
\caption{Speed versus Accuracy}
\end{figure}

\begin{figure}
\centerline{\includegraphics[scale=0.65]{macroF1.png}}
\caption{Speed versus Macro F1 }
\end{figure}

\begin{figure}
\centerline{\includegraphics[scale=0.65]{weightedF1.png}}
\caption{Speed versus Weighted F1 }
\end{figure}

\newpage

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\hypertarget{ref-boehmke2019hands}{}%
Boehmke, B. \& Greenwell, B.M. 2019. \emph{Hands-on machine learning
with r}. CRC Press.

\leavevmode\hypertarget{ref-leibbrandt2010trends}{}%
Leibbrandt, M., Woolard, I., Finn, A. \& Argent, J. 2010. Trends in
south african income distribution and poverty since the fall of
apartheid.

\leavevmode\hypertarget{ref-stats2011social}{}%
Stats, S. 2011. Social profile of vulnerable groups in south africa
2002-2010. \emph{Pretoria: Government Printer}.

\leavevmode\hypertarget{ref-vanderberg2017}{}%
Yu, D. \& Van der Berg, S. 2017. South african poverty: The current
situation and trends since the transition to democracy.

\end{CSLReferences}

\hypertarget{appendix}{%
\section*{Appendix}\label{appendix}}
\addcontentsline{toc}{section}{Appendix}

\hypertarget{appendix-a}{%
\subsection*{Appendix A}\label{appendix-a}}
\addcontentsline{toc}{subsection}{Appendix A}

Some appendix information here

\hypertarget{appendix-b}{%
\subsection*{Appendix B}\label{appendix-b}}
\addcontentsline{toc}{subsection}{Appendix B}

\bibliography{Tex/ref}





\end{document}
