\documentclass[11pt,preprint, authoryear]{elsarticle}

\usepackage{lmodern}
%%%% My spacing
\usepackage{setspace}
\setstretch{1.2}
\DeclareMathSizes{12}{14}{10}{10}

% Wrap around which gives all figures included the [H] command, or places it "here". This can be tedious to code in Rmarkdown.
\usepackage{float}
\let\origfigure\figure
\let\endorigfigure\endfigure
\renewenvironment{figure}[1][2] {
    \expandafter\origfigure\expandafter[H]
} {
    \endorigfigure
}

\let\origtable\table
\let\endorigtable\endtable
\renewenvironment{table}[1][2] {
    \expandafter\origtable\expandafter[H]
} {
    \endorigtable
}


\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{â‚¬}
\fi

\usepackage{amssymb, amsmath, amsthm, amsfonts}

\def\bibsection{\section*{References}} %%% Make "References" appear before bibliography


\usepackage[round]{natbib}

\usepackage{longtable}
\usepackage[margin=2.3cm,bottom=2cm,top=2.5cm, includefoot]{geometry}
\usepackage{fancyhdr}
\usepackage[bottom, hang, flushmargin]{footmisc}
\usepackage{graphicx}
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\numberwithin{table}{section}
\setlength{\parindent}{0cm}
\setlength{\parskip}{1.3ex plus 0.5ex minus 0.3ex}
\usepackage{textcomp}
\renewcommand{\headrulewidth}{0.2pt}
\renewcommand{\footrulewidth}{0.3pt}

\usepackage{array}
\newcolumntype{x}[1]{>{\centering\arraybackslash\hspace{0pt}}p{#1}}

%%%%  Remove the "preprint submitted to" part. Don't worry about this either, it just looks better without it:
\makeatletter
\def\ps@pprintTitle{%
  \let\@oddhead\@empty
  \let\@evenhead\@empty
  \let\@oddfoot\@empty
  \let\@evenfoot\@oddfoot
}
\makeatother

 \def\tightlist{} % This allows for subbullets!

\usepackage{hyperref}
\hypersetup{breaklinks=true,
            bookmarks=true,
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=blue,
            pdfborder={0 0 0}}


% The following packages allow huxtable to work:
\usepackage{siunitx}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{calc}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{caption}


\newenvironment{columns}[1][]{}{}

\newenvironment{column}[1]{\begin{minipage}{#1}\ignorespaces}{%
\end{minipage}
\ifhmode\unskip\fi
\aftergroup\useignorespacesandallpars}

\def\useignorespacesandallpars#1\ignorespaces\fi{%
#1\fi\ignorespacesandallpars}

\makeatletter
\def\ignorespacesandallpars{%
  \@ifnextchar\par
    {\expandafter\ignorespacesandallpars\@gobble}%
    {}%
}
\makeatother

\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newenvironment{CSLReferences}%
  {\setlength{\parindent}{0pt}%
  \everypar{\setlength{\hangindent}{\cslhangindent}}\ignorespaces}%
  {\par}


\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{5}

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}

%%% Include extra packages specified by user
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}

%%% Hard setting column skips for reports - this ensures greater consistency and control over the length settings in the document.
%% page layout
%% paragraphs
\setlength{\baselineskip}{12pt plus 0pt minus 0pt}
\setlength{\parskip}{12pt plus 0pt minus 0pt}
\setlength{\parindent}{0pt plus 0pt minus 0pt}
%% floats
\setlength{\floatsep}{12pt plus 0 pt minus 0pt}
\setlength{\textfloatsep}{20pt plus 0pt minus 0pt}
\setlength{\intextsep}{14pt plus 0pt minus 0pt}
\setlength{\dbltextfloatsep}{20pt plus 0pt minus 0pt}
\setlength{\dblfloatsep}{14pt plus 0pt minus 0pt}
%% maths
\setlength{\abovedisplayskip}{12pt plus 0pt minus 0pt}
\setlength{\belowdisplayskip}{12pt plus 0pt minus 0pt}
%% lists
\setlength{\topsep}{10pt plus 0pt minus 0pt}
\setlength{\partopsep}{3pt plus 0pt minus 0pt}
\setlength{\itemsep}{5pt plus 0pt minus 0pt}
\setlength{\labelsep}{8mm plus 0mm minus 0mm}
\setlength{\parsep}{\the\parskip}
\setlength{\listparindent}{\the\parindent}
%% verbatim
\setlength{\fboxsep}{5pt plus 0pt minus 0pt}



\begin{document}



\begin{frontmatter}  %

\title{Predicting Poverty levels in South Africa}

% Set to FALSE if wanting to remove title (for submission)




\author[Add1]{Jessica van der Berg}
\ead{20190565@sun.ac.za}





\address[Add1]{Stellenbosch University, South Africa}


\begin{abstract}
\small{
Abstract to be written here. The abstract should not be too long and
should provide the reader with a good understanding what you are writing
about. Academic papers are not like novels where you keep the reader in
suspense. To be effective in getting others to read your paper, be as
open and concise about your findings here as possible. Ideally, upon
reading your abstract, the reader should feel he / she must read your
paper in entirety.
}
\end{abstract}

\vspace{1cm}

\begin{keyword}
\footnotesize{
Multivariate GARCH \sep Kalman Filter \sep Copula \\ \vspace{0.3cm}
\textit{JEL classification} L250 \sep L100
}
\end{keyword}
\vspace{0.5cm}
\end{frontmatter}



%________________________
% Header and Footers
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{fancy}
\chead{}
\rhead{}
\lfoot{}
\rfoot{\footnotesize Page \thepage}
\lhead{}
%\rfoot{\footnotesize Page \thepage } % "e.g. Page 2"
\cfoot{}

%\setlength\headheight{30pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%________________________

\headsep 35pt % So that header does not go over title




\hypertarget{introduction}{%
\section{\texorpdfstring{Introduction
\label{Introduction}}{Introduction }}\label{introduction}}

Since apartheid ended in 1994, the South African Government has
committed a significant number of resources to distribute grants
effectively and efficiently to the poor and the vulnerable. However,
South Africa has remained a country with extremely high levels of
inequality and poverty for an upper middle-income country
\protect\hyperlink{ref-stats2011social}{Stats}
(\protect\hyperlink{ref-stats2011social}{2011}). Previous literature has
shown that poverty levels are higher in rural areas than in urban areas.
This is largely due to rural household not having access to the same
employment opportunities has urban households. South Africa is a country
with high levels of unemployment and extremely low wages. This implies
that individuals that are economically active can still fall below the
upper or lower bound poverty line due to the low wages that they receive
\protect\hyperlink{ref-leibbrandt2010trends}{Leibbrandt, Woolard, Finn
\& Argent} (\protect\hyperlink{ref-leibbrandt2010trends}{2010}).

Poverty remains unacceptably high for a country of South Africa's
economic status and remains closely associated with race. Thus, poverty
reduction remains one of the key economic goals. Poverty in money terms
has declined markedly since apartheid ended in 1994. This was made
possible by the expansion of the social grant system. However,
accurately targeting social welfare programs can be challenging given
that income data is often incorrect
\protect\hyperlink{ref-vanderberg2017}{Yu \& Van der Berg}
(\protect\hyperlink{ref-vanderberg2017}{2017}). To overcome this
problem, households are subject to a proxy means test (PMT) to identify
whether a household qualifies for social assistant. This essay will try
to identify new methods to identify households that are below the food
poverty line, lower-bound poverty line and upper bound poverty line
using machine learning techniques.

Structure \ldots{} `

\hypertarget{literature-review}{%
\section{Literature Review}\label{literature-review}}

Write a short literature review here on the background of SOuth Africa -
max one page

\hypertarget{data}{%
\section{Data}\label{data}}

The data used for predicting poverty level was exacted from the General
Household Survey (GHS) 2018, which is a survey completed annually by
Statistics SA to measure the living circumstances of households in South
Africa. They survey includes household and individual characteristics.
After taking out all of the NA values and taking out households who did
not know or answer the relevant questions, the dataset consists of 14
546 entries.

Since the GHS consist of household survey, total income per household is
reported. To calculate the average monthly income per individual within
each household, I first need to calculate the total number of adults
within each household. This is done by taking the difference between
household size and the number of children under the age of 17. I then
take the total monthly income and divide it by the total number of
adults to get the average monthly income per individual in each
household. This income information is then used to divided the data into
four groups.

The first group is individuals whose income fall below the food poverty
line. In 2018, the food poverty line was R547 per month. The food
poverty line is also referred to as the extreme poverty line as it
refers to the absolute minimum amount an individual will need to be able
to afford the minimum energy intake for survival. The second group
consist of individuals whose monthly income falls between the food
poverty line and the lower-bound poverty line (R785). The lower bound
poverty line is the sum of the food poverty line and and minimum amount
for non-food items. The third group consist of individuals between the
lower-bound poverty line and the upper-bound poverty line (R1183). The
final group consist of individuals whose monthly income is above the
upper-bound poverty line, which I refer to as non-vulnerable
individuals. Figure 1 graphically displays the process described above
in the format of a decision tree, where 1 represents the food poverty
line, 2 the lower-bound poverty line, 3 the upper-bound poverty line and
4 the non-vulnerable.

\begin{figure}[H]

{\centering \includegraphics{Predicting-Poverty_files/figure-latex/Figure1-1} 

}

\caption{Decision tree for poverty levels \label{Figure1}}\label{fig:Figure1}
\end{figure}

After all data cleaning was done, 73 percent of households were
non-vulnerable, 11 percent were between the upper and lower-bound
poverty line, 6 percent between the lower and food poverty line, and 9
percent fell below the food poverty line. To evaluate the performance
for the machine learning techniques that I will implement, I randomly
split the data into two subsets using a 70:30 ratio. The training
dataset will consist of 70 percent of the original dataset, while the
test dataset will consist of the remaining 30 percent. Figure 2 below
shows the number of household per poverty level for the training
dataset.

\begin{figure}
\centerline{\includegraphics[scale=0.65]{plot1.png}}
\caption{Number of household per poverty level}
\end{figure}

Figure 2 shows that in our training dataset, 73.5 percent of households
are non-vulnerable and 9.6 percent of households monthly income falls
below the food poverty line. This implies that I have an extremely
imbalanced dataset. This is important as it will have an effect on the
machine learning techniques that I implement later on.

For some descriptive statistics, I analyze some of the variable in more
details. From the density plots in figure 3.3, I can see that households
whose average income per person falls below the upper-bound poverty line
(green line) tend to have slightly older head of household (head\_age)
then those household belonging to other poverty levels.

A household size (hholdsz) of 1-2 individuals tend have a larger
probability to be non-vulnerable (yellow line), where larger household
size of 3-5 individuals then have a larger probability of falling below
the food poverty line.

Total expenditure (Q814Exp) is higher for the non-vulnerable households,
which makes sense as they receive a higher income.

The total number of rooms (Q55TotRm) seem to be distributed similarly,
with non-vulnerable households having a slightly smaller distribution.
This comes as no suprise as these non-vulnerable households tend to have
a smaller household size, implying that they need less bedrooms.

Non-vulnerable household also have much larger property valuation
(Q58Val), which could imply that they have a higher standard of living.

\begin{figure}[H]

{\centering \includegraphics{Predicting-Poverty_files/figure-latex/Figure3-1} 

}

\caption{Distribution of certain variables by poverty level \label{Figure3}}\label{fig:Figure3}
\end{figure}

Before we start building models we can also analysis which variables are
correlated with our dependent variable (which is is poverty level).
Figure 3.4 displays the top 10 variables that are correlated with the
dependent variable. Our top four that are positively correlated are
\textit{econact hh}, which is a binary variable indicating whether or
not the household is economically active, \emph{Q814Exp}, which is a
numeric variable display total household expenditure, \emph{totmhinc},
which is a numeric variable displaying total household income, and
\emph{Q89aGrant}, which is a binary variable indicating whether the
household receives a grant or not.

variables that are negatively correlated with the dependent variable is
the sex of the head of the household (\emph{head sex}), whether or not
the household owns a washing machine (\emph{Q821WashM}), the household
size (\emph{hholdsz}), the amount spent on rent or mortgage of property
per month (\emph{Q57Rent}) and the number of adults in the household
(\emph{adults}).

\begin{figure}[H]

{\centering \includegraphics{Predicting-Poverty_files/figure-latex/Figure4-1} 

}

\caption{Correlation with dependent variable \label{Figure4}}\label{fig:Figure4}
\end{figure}

\hypertarget{methodology}{%
\section{\texorpdfstring{Methodology
\label{Meth}}{Methodology }}\label{methodology}}

\hypertarget{analysing-classification-models}{%
\subsection{Analysing Classification
Models}\label{analysing-classification-models}}

To see which model predicts poverty most accurately and efficiently, I
assess the performance of seven supervised classification models using
the caret package. The dataset is extremely unbalanced; therefore, the
defaults parameter will be Kappa to order to improve the performance of
the models. The Kappa metric compares the observed accuracy with the
expected accuracy. It also accounts for random chance which implies that
it makes the model more accurate that simply using an Accuracy as a
metric. The Kappa matric is calculated using the formula;
\[k = \frac{p_0 -p_e}{1-p_e}\] Where \(p_0\) represents the overall
accuracy of the model and \(p_e\) represents that measure of the
agreement between the predictions and actual class value of the model.
Therefore, Kappa attempts to account for evaluation bias by considering
the correct classification by a random guess
\protect\hyperlink{ref-dalpiaz}{Dalpiaz}
(\protect\hyperlink{ref-dalpiaz}{2017}).

The table below briefly discusses the seven different classification
models that I compared to determine which model is the most accurate to
predict poverty \protect\hyperlink{ref-boehmke2019hands}{Boehmke \&
Greenwell} (\protect\hyperlink{ref-boehmke2019hands}{2019}).

\begin{center}
\begin{tabular}{| m{10em} | m{2cm}| m{10cm} |}
\hline
\textbf{Model} & \textbf{Reference Name} & \textbf{Description} \\
\hline
Multinomial Logistic Regression &  multinom & Makes use of the maximum likelihood estimation to evaluate the probability of a categorical relationship \\
\hline
Linear Discriminant Analysis & lda & Used to find linear combinations of separates multiple classes of features, representing the dependent variable as a linear combination of other features \\
\hline
Naive Bayes & naive bayes & Using Bayes Theorem, this model applies posterior probability to the categorization, making the uneducated assumption that the predictors are independent \\
\hline
Linear Support Vector Machine & svmLinear & The model creates a line that separates data into classes \\
\hline
K-Nearest Neighbor & knn & Each observation in the dataset is predicted based on its similarity to other observations \\
\hline
Recursive Partitioning & rpart & Builds models using a general structure which consist of a two-stage procedure and then final presenting the model as a binary tree \\
\hline 
Ranger & ranger & An updated and fast implementation of random forest for big data \\
\hline
\end{tabular}
\end{center}

\hypertarget{evaluation}{%
\subsubsection{Evaluation}\label{evaluation}}

To ensure that the correct model is chosen, the accuracy of each model
is evaluated and compared. However, since the dataset is unbalance, I
also analyze the F measure, also known as the \(F_1\) score, of each
model. The \(F_1\) score communicates the average between the precision
and the recall of each model. A perfect model has an \(F_1\) score equal
to 1, therefore models with a higher \(F_1\) score is preferred over
models with a lower \(F_1\) score. The formula for the \(F_1\) score is
given below;
\[F_1 = 2\times \frac{ precision \times recall}{precision + recall}\]

The extremely unbalanced data will affect the results of the \(F_1\)
score. Therefore, it is also informative to evaluate the macro \(F_1\)
score and the weighted \(F_1\) score. The macro \(F_1\) is not affected
by unbalanced data and is equal to the average of the \(F_1\) score and
is commonly used when there are multiple levels or classes. It gives the
same importance to each poverty level. A higher macro \(F_1\) score is
preferred to a lower one. The formula for the macro \(F_1\) score if
given below; \[ Macro\: F_1\: score = \frac{1}{N}{\sum}_{i=0}^N F_1\]

Where \(N\) is the number of different poverty levels and \(i\) is the
levels index. The drawback of the macro \(F_1\) measure is that is gives
equal weight to all poverty levels, which implies that it over emphasis
the under-represented poverty levels. The weighted average \(F_1\) score
is similar to the macro \(F_1\) score, but here the \(F_1\) score is
weighed according of the number of households from the specific poverty
level, which emphasis poverty levels according to size of each poverty
level. The formula for the weighted \(F_1\) score is given below;
\[ Weighted\: F_1\: score = \frac{n_i\sum_{i=1}^k F_1}{\sum_{i=n}^k n_i}\]

\hypertarget{decision-tree}{%
\subsection{Decision Tree}\label{decision-tree}}

As I will show in section 5.1, decision trees have the fastest
computational time without having to compromise much on the accuracy of
a model. Decision trees are constructed through an algorithmic approach
that identifies the most optimal way to split a dataset based on the
information in the dataset. Decision trees are displayed in a
flowchart-like structure where each internal node represents some sort
of test on a specific feature. Each leaf node then represents a poverty
level. The path from the root (the first node) to the leaf represents
the classification rules. Decision trees are relatively easy to
interpret and therefore, are commonly used
\protect\hyperlink{ref-boehmke2019hands}{Boehmke \& Greenwell}
(\protect\hyperlink{ref-boehmke2019hands}{2019}).

\hypertarget{random-forest}{%
\subsection{Random Forest}\label{random-forest}}

In section 5.1, I also show that random forest provides perfect accuracy
out of all the classification models that are considered, however, it
also has the longest computational time. Random forest uses multiple
decision trees to provide more flexibility and better accuracy, while
reaching a single result. Random forest searches for the best feature
form a random subset of features which leads to it providing more
randomness to the model. The increased in randomness is what improves
the model accuracy as it ensures a low correlation among the multiple
decision trees \protect\hyperlink{ref-breiman2015random}{Breiman}
(\protect\hyperlink{ref-breiman2015random}{2015}).

\hypertarget{results-and-discussion}{%
\section{Results and Discussion}\label{results-and-discussion}}

\hypertarget{classification-models}{%
\subsection{Classification Models}\label{classification-models}}

The table below shows the different metrics to make comparing different
models easier. The first feature that is observed is that the models
vary drastically in time. The \emph{lda}, \emph{rpart} and
\emph{naive bayes} models are extremely fast whereas \emph{multinom} and
\emph{ranger} take relatively long to run. Furthermore, \emph{ranger}
scores the best for accuracy and \(F_1\) measures.

\begin{center}
\begin{tabular}{| c| c| c |c| c|}
\hline
\textbf{Models} & \textbf{Time} & \textbf{Accuracy} & \textbf{Macro \(F_1\)} & \textbf{weighted \(F_1\)} \\
\hline
multinom & 37.17 & 0.9935839 & 0.9820473 & 0.9935798 \\
\hline
lda & 0.81 & 0.7774977 & 0.4561088 & 0.7409658 \\
\hline
naive bayes & 1.67 & 0.8819890 & 0.8138291 & 0.8862441 \\
\hline
svmLinear & 9.91 & 0.9660862 & 0.9134729 & 0.9661192 \\
\hline
knn & 4.81 & 0.9869386 & 0.9714937 & 0.9876236 \\
\hline 
rpart & 1.08 & 0.9372136 & 0.9155884 & 0.9137702 \\
\hline 
ranger & 41.92 & 1.00000000 & 1.00000000 & 1.00000000 \\
\hline
\end{tabular}
\end{center}

Analyzing the accuracy score, Figure 5.1 shows that \emph{ranger} has
the highest accuracy, followed closely by \emph{multinom}. However,
these two models also take the longest to run. \emph{knn} and
\emph{rpart} are relatively fast and they have a high level of accuracy.
For the fastest running models, decision trees (\emph{rpart}) are the
most accurate. Figure 5.1 also implies that there is some trade-off
between accuracy and computational time.

\begin{figure}
\centerline{\includegraphics[scale=0.65]{accuracy.png}}
\caption{Speed versus Accuracy}
\end{figure}

Figure 5.2 shows the relationship between computational time and the
Macro \(F_1\) score measure. \emph{ranger} and \emph{multinom} still
have the highest degree of accuracy. However, \emph{rpart} has a higher
degree of accuracy while maintain the same computational time.
\emph{naive bayes} is performing worst in terms of accuracy, while
\emph{knn}, \emph{svmLinear} and \emph{lda} are all performing
relatively the same. Here, the same conclusion is reached as with Figure
5.1, that decision trees (\emph{rpart}) is the most accurate when
analyzing models with the fastest computational time.

\begin{figure}
\centerline{\includegraphics[scale=0.65]{macroF1.png}}
\caption{Speed versus Macro F1 }
\end{figure}

Figure 5.3 displays that \emph{naive bayes} preforms slightly better in
terms of accuracy when compared to the macro \(F_1\) score whereas the
rest of the models preform similarly. This suggest that random forest
(\emph{ranger}) is the most accurate when predicting poverty across all
the measures, however, the computational time is extensive. If you are
willing to compromise on the accuracy of a model, then decision tree
(\emph{rpart}) is the best model, which serves a high degree of accuracy
and an extremely fast computational time. Now that I have determined
which models are the best to predict poverty, decisions tree and random
forest will be analyzed.

\begin{figure}
\centerline{\includegraphics[scale=0.65]{weightedF1.png}}
\caption{Speed versus Weighted F1 }
\end{figure}

\hypertarget{decision-tree-1}{%
\subsection{Decision Tree}\label{decision-tree-1}}

The decision tree in Figure 5.4 shows the path of classification rules
that determine under which poverty level a household is classified,
where red (1) represents household falling beneath the food poverty
line, orange (2) represents households that are above the food poverty
line but below the lower-bound poverty line, purple (3) represents
households that are above the lower-bound poverty line but below the
upper-bound poverty line and green (4) represents households that are
above the upper-bound poverty line.

As we can see, household income (\emph{totmhinc}), the number of adults
in the household (\emph{adults}) and monthly salary
(\textbackslash emph\{Q42Msal\_hh\}) are the only variables used to
determine the poverty level of a household. The difference between
monthly salary and total household income is that total household income
consists of wages/salary, grants and any other type of income a
household might receive, whereas monthly salary only consists of money
received through employment. Figure 5.4 communicates that income
variables and the number of adults are the most important variables when
determining the poverty level under which households fall.

\begin{figure}[H]

{\centering \includegraphics{Predicting-Poverty_files/figure-latex/Figure7-1} 

}

\caption{Decision Tree\label{Figure7}}\label{fig:Figure7}
\end{figure}

If a household's total monthly income is below R2 375, then there is
only a 7 percent chance that the household will earn enough to be above
the upper-bound poverty line, and this is only for households that
consist of only one adult. Therefore, a single adult can earn between R
1190 and R2375 and still be above the upper-bound poverty line.
Households of more than 4 adults tend to be poorer than smaller
households. This statement supports the findings of @ lanjouw1995poverty
that larger households are more likely to be poorer in developing
countries. To further assess which variables, determine under which
poverty level a household falls, I construct a decision tree not
considering any income variables. This is displayed below in Figure 5.5.

\begin{figure}[H]

{\centering \includegraphics{Predicting-Poverty_files/figure-latex/Figure8-1} 

}

\caption{Decision Tree excluding income variables\label{Figure8}}\label{fig:Figure8}
\end{figure}

Now that income variables are excluded, a different picture of variables
affecting poverty levels are displayed. The first thing we noticed is
that all households are economically active are above the upper-bound
poverty level. This means the employment is a big determinant of the
poverty level a household falls under and that creating more jobs can
help reduce poverty.

Furthermore, households that have a head of younger than 60 years of
age, tend to be better off than households where the head is older than
60 years. This implies that old age grants, such as pension grants,
could be very important source of income to poorer households. Pension
grants provide a regular income to poor households, meaning that it has
the potential to reduce poverty.
\protect\hyperlink{ref-duflo2003}{Duflo}
(\protect\hyperlink{ref-duflo2003}{2003}) found that pension grants also
have a positive effect on the nutrition and health of young girls.
Therefore, pension grants can reduce poverty as well as improve the
health on children. It also seems that poorer households receive more
than two grants, however, the grants fail to keep the household above
the food-poverty line.

\newpage

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\hypertarget{ref-boehmke2019hands}{}%
Boehmke, B. \& Greenwell, B.M. 2019. \emph{Hands-on machine learning
with r}. CRC Press.

\leavevmode\hypertarget{ref-breiman2015random}{}%
Breiman, L. 2015. Random forests leo breiman and adele cutler.
\emph{Random Forests-Classification Description}.

\leavevmode\hypertarget{ref-dalpiaz}{}%
Dalpiaz, D. 2017.

\leavevmode\hypertarget{ref-duflo2003}{}%
Duflo, E. 2003. Grandmothers and granddaughters: Old-age pensions and
intrahousehold allocation in south africa. \emph{The World Bank Economic
Review}. 17(1):1--25.

\leavevmode\hypertarget{ref-leibbrandt2010trends}{}%
Leibbrandt, M., Woolard, I., Finn, A. \& Argent, J. 2010. Trends in
south african income distribution and poverty since the fall of
apartheid.

\leavevmode\hypertarget{ref-stats2011social}{}%
Stats, S. 2011. Social profile of vulnerable groups in south africa
2002-2010. \emph{Pretoria: Government Printer}.

\leavevmode\hypertarget{ref-vanderberg2017}{}%
Yu, D. \& Van der Berg, S. 2017. South african poverty: The current
situation and trends since the transition to democracy.

\end{CSLReferences}

\hypertarget{appendix}{%
\section*{Appendix}\label{appendix}}
\addcontentsline{toc}{section}{Appendix}

\hypertarget{appendix-a}{%
\subsection*{Appendix A}\label{appendix-a}}
\addcontentsline{toc}{subsection}{Appendix A}

Some appendix information here

\hypertarget{appendix-b}{%
\subsection*{Appendix B}\label{appendix-b}}
\addcontentsline{toc}{subsection}{Appendix B}

\bibliography{Tex/ref}





\end{document}
